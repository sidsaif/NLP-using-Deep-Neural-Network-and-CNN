{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "QnkWj_5dbMTf"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "dLddoHPNbMTp",
    "outputId": "027058e3-e9bd-496c-baa0-51d1707d865a"
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "ajH_DeTVbMTv",
    "outputId": "010d1829-60ff-402b-a883-f05133e4cf98"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('/content/drive/My Drive/Overall_Train_data.xlsx')\n",
    "#df = df.sample(n=100000, random_state=134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2rDT7Uv7I_IO"
   },
   "outputs": [],
   "source": [
    "df.head()\n",
    "#df.columns = ['id', 'Verbatim Response', 'IMPROVEMENT', 'LIKE', 'NEUTRAL']\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "rCreozG_bMT1",
    "outputId": "a83545f9-7e10-4626-84bf-690724d14181"
   },
   "outputs": [],
   "source": [
    "df['Verbatim_Cleaned'] = df['Verbatim_Cleaned'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rf9QgIL4bMUE",
    "outputId": "eac7f317-008e-49df-e222-67a7aac7e141"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "def tokenize(text):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n",
    "#df['Sentences'] = df['Verbatim_Cleaned'].apply(tokenize)\n",
    "#df['n_words'] = [len(i) for i in df['Sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_t3LN-DWbMUG"
   },
   "outputs": [],
   "source": [
    "#df = df.drop(['Sentences', 'n_words'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "qYDXZ9bgbMUJ",
    "outputId": "91134408-d270-4bb2-d627-851313778cd3"
   },
   "outputs": [],
   "source": [
    "############### PreProcessing ###############\n",
    "import re \n",
    "ONLY_DIGITS = re.compile('^[0-9]+$')  #Remove verbatims which have no words\n",
    "def preprocess_text(sen):\n",
    "    # Remove digits only sentences\n",
    "    #sentence = ONLY_DIGITS.sub('', sen)\n",
    "    sentence = str(sen).replace(\"\\d+\", '')\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "X = df['Verbatim_Cleaned']\n",
    "x = []\n",
    "sentences = list(X)\n",
    "for sen in sentences:\n",
    "    x.append(preprocess_text(sen))\n",
    "\n",
    "y = df.iloc[:,5:26].values\n",
    "\n",
    "\n",
    "unique_words = df['Verbatim_Cleaned'].str.split(expand=True).stack().value_counts() \n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Ab_kc_ljCWAx",
    "outputId": "7bca7cbc-35a4-4859-dc68-a77c65260f5f"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OWTEvBRrbMUL"
   },
   "outputs": [],
   "source": [
    "################   Setting Parameter\n",
    "#the path for Glove embeddings\n",
    "GLOVE_DIR = '/content/drive/My Drive/bert/'\n",
    "# make the max word length to be constant\n",
    "MAX_WORDS = len(unique_words)\n",
    "MAX_SEQUENCE_LENGTH = 700\n",
    "# the percentage of train test split to be applied\n",
    "VALIDATION_SPLIT = 0.20\n",
    "# the dimension of vectors to be used\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "uWDOWpk9bMUO",
    "outputId": "9a7992ec-b839-4f56-d71b-1f10247d7804"
   },
   "outputs": [],
   "source": [
    "#################  Tokenization and Padding ####################\n",
    "tokenizer  = Tokenizer(num_words = MAX_WORDS)\n",
    "tokenizer.fit_on_texts(x)\n",
    "sequences =  tokenizer.texts_to_sequences(x)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"unique words : {}\".format(len(word_index)))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "labels = y\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label Subcategory tensor:', labels.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qlIZ6966bMUR",
    "outputId": "c7b1a018-2ad1-49ab-df42-1a9eed7a5fe8"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hUK9pg0abMUU",
    "outputId": "01959066-ffd8-4215-e130-06173098b165"
   },
   "outputs": [],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('/content/drive/My Drive/bert/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rKn8RYBrbMUX"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data,labels,test_size=0.25, random_state = 342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MuOd-ka-dhFC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "SALuvGO2bMUa",
    "outputId": "d83316cd-7823-462f-dd5d-ae09085f6af4"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.python.keras.losses import BinaryCrossentropy\n",
    "\n",
    "model_sentiment = Sequential()\n",
    "model_sentiment.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=MAX_SEQUENCE_LENGTH, \n",
    "                           trainable=True))\n",
    "model_sentiment.add(layers.Flatten())\n",
    "#model.add(layers.Dropout(0.3))\n",
    "model_sentiment.add(layers.Dense(500, activation='relu'))\n",
    "model_sentiment.add(layers.Dense(500, activation='relu'))\n",
    "model_sentiment.add(layers.Dense(21, activation='sigmoid'))\n",
    "model_sentiment.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "model_sentiment.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "r8EiW1RabMUc",
    "outputId": "24540fb4-caeb-4b6c-b463-bcab0e1d7b3c"
   },
   "outputs": [],
   "source": [
    "history = model_sentiment.fit(X_train, Y_train,\n",
    "                    epochs=5,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.2,\n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "o1qY_93KbMUd",
    "outputId": "2242ff4c-9d2a-44bc-c17d-a4ac67591ce4"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_train, Y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, Y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "colab_type": "code",
    "id": "aU4PJ_wkbMUf",
    "outputId": "409c3a92-f3bc-41d7-81c8-8c0d90868679"
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)\n",
    "predicted_train = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "crQSPoL6bMUg"
   },
   "outputs": [],
   "source": [
    "predicted_class = np.round(predicted)\n",
    "predicted_train = np.round(predicted_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "H9NcXNKIbMUj",
    "outputId": "924d70db-dea8-4be3-91a7-04a3b4ebb31a"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print(\"Accuracy:\", accuracy_score(Y_test, predicted_class))\n",
    "print(\"f1_score\", f1_score(Y_test, predicted_class, average = 'micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0z2wFZ5ybMUm"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, fbeta_score, precision_score, zero_one_loss, hamming_loss, make_scorer, roc_auc_score, average_precision_score, jaccard_similarity_score, log_loss, precision_recall_fscore_support, recall_score\n",
    "from sklearn.metrics import coverage_error, label_ranking_average_precision_score, label_ranking_loss \n",
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    '''\n",
    "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
    "    https://stackoverflow.com/q/32239577/395857\n",
    "    '''\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        #print('\\nset_true: {0}'.format(set_true))\n",
    "        #print('set_pred: {0}'.format(set_pred))\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        #print('tmp_a: {0}'.format(tmp_a))\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)\n",
    "\n",
    "def print_accuracy_score_train(y_pred):\n",
    "    print(\"Hamming score: {}\".format(hamming_score(y_train, y_pred)))\n",
    "    print('Hamming loss: {0}'.format(hamming_loss(y_train, y_pred))) \n",
    "    print(\"Accuracy Score: {}\".format(accuracy_score(y_train, y_pred, normalize=True, sample_weight=None)))\n",
    "\n",
    "def print_accuracy_score_test(y_pred):\n",
    "    print(\"Hamming score: {}\".format(hamming_score(Y_test, y_pred)))\n",
    "    print('Hamming loss: {0}'.format(hamming_loss(Y_test, y_pred))) \n",
    "    print(\"Accuracy Score: {}\".format(accuracy_score(Y_test, y_pred, normalize=True, sample_weight=None)))\n",
    "    print(\"Precision Score: {}\".format(precision_score(Y_test, y_pred, average = 'micro')))\n",
    "    print(\"Recall Score: {}\".format(recall_score(Y_test, y_pred, average = 'micro')))\n",
    "    print(\"F1 Score: {}\".format(f1_score(Y_test, y_pred, average = 'micro')))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AW8so6OPbMUp"
   },
   "outputs": [],
   "source": [
    "print_accuracy_score_train(predicted_train)\n",
    "print_accuracy_score_test(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "hj0aJQfblsjS",
    "outputId": "1112e4fb-c422-4bf8-94b6-e03e038a90b5"
   },
   "outputs": [],
   "source": [
    "#### Model for SubCategory ###\n",
    "model_subcat = Sequential()\n",
    "model_subcat.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=MAX_SEQUENCE_LENGTH, \n",
    "                           trainable=False))\n",
    "model_subcat.add(layers.Flatten())\n",
    "#model.add(layers.Dropout(0.3))\n",
    "model_subcat.add(layers.Dense(500, activation='relu'))\n",
    "model_subcat.add(layers.Dense(500, activation='relu'))\n",
    "model_subcat.add(layers.Dense(219, activation='sigmoid'))\n",
    "model_subcat.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "model_subcat.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "0Wy90DJvlsg9",
    "outputId": "2cc7b155-8880-4a4c-a134-0811f05a7f93"
   },
   "outputs": [],
   "source": [
    "history = model_subcat.fit(x_train, y_train_subcat,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val_subcat),\n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ySTEno6Llsd2",
    "outputId": "07fc760f-e8ec-4f6f-f6e8-2e647eb86905"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model_subcat.evaluate(x_train, y_train_subcat, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_subcat.evaluate(X_test, Y_test_subcat, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HJPYmaQmome7"
   },
   "outputs": [],
   "source": [
    "predicted = model_subcat.predict(X_test)\n",
    "predicted_train = model_subcat.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gq-AXuCqomba"
   },
   "outputs": [],
   "source": [
    "predicted_class = np.round(predicted)\n",
    "predicted_train = np.round(predicted_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "0a49KcynomUi",
    "outputId": "0f58f054-5957-42cd-f834-1d3bb0eb7c5c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print(\"Accuracy:\", accuracy_score(Y_test_sent, predicted_class))\n",
    "print(\"f1_score\", f1_score(Y_test_sent, predicted_class, average = 'micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqZsSEoHho9e"
   },
   "outputs": [],
   "source": [
    "test_new = df.sample(n=20000, random_state=343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FbfxRUbohrEv"
   },
   "outputs": [],
   "source": [
    "y_test_new = test_new.iloc[:,1:].values\n",
    "X_test_new = test_new['Verbatim Response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pqFSvErViHc5",
    "outputId": "a74c5fc2-a8a0-42fd-ba1d-2f35577e30aa"
   },
   "outputs": [],
   "source": [
    "############### PreProcessing ###############\n",
    "import re \n",
    "def preprocess_text(sen):\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "x = []\n",
    "sentences = list(X)\n",
    "for sen in sentences:\n",
    "    x.append(preprocess_text(sen))\n",
    "\n",
    "\n",
    "unique_words = X_test_new.str.split(expand=True).stack().value_counts() \n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P76poFDTichx"
   },
   "outputs": [],
   "source": [
    "#################  Tokenization and Padding ####################\n",
    "sequences =  tokenizer.texts_to_sequences(X_test_new)\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PghfI-H4i8jj"
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yr7Bdp2UjL9s"
   },
   "outputs": [],
   "source": [
    "predicted_class = np.round(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "KVxBl7QAjM36",
    "outputId": "6e13ef1d-b93c-4458-d674-7e438f872649"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test_new, predicted_class))\n",
    "print(\"f1_score\", f1_score(y_test_new, predicted_class, average = 'micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCSKpLexn5d-"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Activation, Conv2D, Input, Embedding, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D,LSTM, AveragePooling2D\n",
    "from keras.layers import MaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nm-GaGUvoQvP"
   },
   "outputs": [],
   "source": [
    "# filter sizes of the different conv layers \n",
    "filter_sizes = [1,2,3]\n",
    "num_filters = 64\n",
    "embedding_dim = 100\n",
    "# dropout probability\n",
    "drop = 0.2\n",
    "batch_size = 30\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQZPBzjVoynP"
   },
   "outputs": [],
   "source": [
    "############# Keras Embedding #######\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            embedding_dim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "I_ju8NzOn5b8",
    "outputId": "e2c82c96-3c60-417a-be15-638424528bf8"
   },
   "outputs": [],
   "source": [
    "#################### Model Structure ###############\n",
    "inputs = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedding = embedding_layer(inputs)\n",
    "\n",
    "print(embedding.shape)\n",
    "reshape = Reshape((MAX_SEQUENCE_LENGTH,embedding_dim,1))(embedding)\n",
    "print(reshape.shape)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(MAX_SEQUENCE_LENGTH - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "#dropout = Dropout(drop)(flatten)\n",
    "layer_1 = Dense(units=500, activation='relu')(flatten)\n",
    "#layer_2 = Dense(units=500, activation='relu')(layer_1)\n",
    "output = Dense(units=21, activation='sigmoid')(layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eOuVLMU8jZ00"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UeYeYkdsn5Ze"
   },
   "outputs": [],
   "source": [
    "# this creates a model that includes\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights_cnn_sentece.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "znuH2tasn5V-",
    "outputId": "455e102a-2f44-469f-ba79-3a5410c96889"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "kW-Et6LUn5Qr",
    "outputId": "ffeaf451-ff75-4c7b-9bdf-cf4a9fc92488"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train\n",
    "                       , Y_train\n",
    "                       , epochs=20\n",
    "                       , batch_size=64\n",
    "                       , validation_split = 0.2\n",
    "                       , verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "Y7q5ZMRaPejm",
    "outputId": "7256070d-0e0f-45aa-9342-85652a876d5d"
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('/content/drive/My Drive/Improvement_Train.xlsx')\n",
    "#df = df.sample(n=100000, random_state=134)\n",
    "\n",
    "df.head()\n",
    "\n",
    "df['Verbatim_Cleaned'] = df['Verbatim_Cleaned'].str.lower()\n",
    "\n",
    "X = df['Verbatim_Cleaned']\n",
    "Y = df.iloc[:,2:]\n",
    "Y.shape\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "def tokenize(text):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n",
    "#df['Sentences'] = df['Verbatim_Cleaned'].apply(tokenize)\n",
    "#df['n_words'] = [len(i) for i in df['Sentences']]\n",
    "\n",
    "#df = df.drop(['Sentences', 'n_words'], axis=1)\n",
    "\n",
    "############### PreProcessing ###############\n",
    "import re \n",
    "ONLY_DIGITS = re.compile('^[0-9]+$')  #Remove verbatims which have no words\n",
    "def preprocess_text(sen):\n",
    "    # Remove digits only sentences\n",
    "    #sentence = ONLY_DIGITS.sub('', sen)\n",
    "    sentence = str(sen).replace(\"\\d+\", '')\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "x = []\n",
    "sentences = list(X)\n",
    "for sen in sentences:\n",
    "    x.append(preprocess_text(sen))\n",
    "\n",
    "y = df.iloc[:,2:].values\n",
    "\n",
    "unique_words = df['Verbatim_Cleaned'].str.split(expand=True).stack().value_counts() \n",
    "print(len(unique_words))\n",
    "\n",
    "################   Setting Parameter\n",
    "#the path for Glove embeddings\n",
    "GLOVE_DIR = '/content/drive/My Drive/bert/'\n",
    "# make the max word length to be constant\n",
    "MAX_WORDS = len(unique_words)\n",
    "MAX_SEQUENCE_LENGTH = 700\n",
    "# the percentage of train test split to be applied\n",
    "VALIDATION_SPLIT = 0.20\n",
    "# the dimension of vectors to be used\n",
    "embedding_dim = 100\n",
    "\n",
    "#################  Tokenization and Padding ####################\n",
    "tokenizer  = Tokenizer(num_words = MAX_WORDS)\n",
    "tokenizer.fit_on_texts(x)\n",
    "sequences =  tokenizer.texts_to_sequences(x)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"unique words : {}\".format(len(word_index)))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "\n",
    "labels = y\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "print(labels)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n",
    "\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('/content/drive/My Drive/bert/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s25Hm-BAPegE"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data,labels,test_size=0.25, random_state = 342)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train,Y_train,test_size=0.2, random_state = 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "id": "UmkqxxF-Peas",
    "outputId": "d39226c1-ab2b-487d-d1d4-930dcb10eb33"
   },
   "outputs": [],
   "source": [
    "model_like = Sequential()\n",
    "model_like.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=MAX_SEQUENCE_LENGTH, \n",
    "                           trainable=False))\n",
    "model_like.add(layers.Flatten())\n",
    "#model.add(layers.Dropout(0.3))\n",
    "model_like.add(layers.Dense(500, activation='relu'))\n",
    "model_like.add(layers.Dense(500, activation='relu'))\n",
    "model_like.add(layers.Dense(161, activation='sigmoid'))\n",
    "model_like.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model_like.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "VVFjiF3xPeXq",
    "outputId": "cabbc934-88e9-452a-8487-82888c4299a8"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model_like.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model_like.evaluate(X_test, Y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "predicted = model_like.predict(X_test)\n",
    "predicted_train = model_like.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "koct8F4JPeUr"
   },
   "outputs": [],
   "source": [
    "predicted_class = np.round(predicted)\n",
    "predicted_train = np.round(predicted_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "01ntAKmNPeM2",
    "outputId": "3f46f77b-c6ef-469e-996d-6fd6a0cd0875"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, fbeta_score, precision_score, zero_one_loss, hamming_loss, make_scorer, roc_auc_score, average_precision_score, jaccard_similarity_score, log_loss, precision_recall_fscore_support, recall_score\n",
    "from sklearn.metrics import coverage_error, label_ranking_average_precision_score, label_ranking_loss \n",
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    '''\n",
    "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
    "    https://stackoverflow.com/q/32239577/395857\n",
    "    '''\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        #print('\\nset_true: {0}'.format(set_true))\n",
    "        #print('set_pred: {0}'.format(set_pred))\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        #print('tmp_a: {0}'.format(tmp_a))\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)\n",
    "\n",
    "def print_accuracy_score_train(y_pred):\n",
    "    print(\"Hamming score: {}\".format(hamming_score(y_train, y_pred)))\n",
    "    print('Hamming loss: {0}'.format(hamming_loss(y_train, y_pred))) \n",
    "    print(\"Accuracy Score: {}\".format(accuracy_score(y_train, y_pred, normalize=True, sample_weight=None)))\n",
    "\n",
    "def print_accuracy_score_test(y_pred):\n",
    "    print(\"Hamming score: {}\".format(hamming_score(Y_test, y_pred)))\n",
    "    print('Hamming loss: {0}'.format(hamming_loss(Y_test, y_pred))) \n",
    "    print(\"Accuracy Score: {}\".format(accuracy_score(Y_test, y_pred, normalize=True, sample_weight=None)))\n",
    "    print(\"Precision Score: {}\".format(precision_score(Y_test, y_pred, average = 'micro')))\n",
    "    print(\"Recall Score: {}\".format(recall_score(Y_test, y_pred, average = 'micro')))\n",
    "    print(\"F1 Score: {}\".format(f1_score(Y_test, y_pred, average = 'micro')))\n",
    "\n",
    "print_accuracy_score_train(predicted_train)\n",
    "print_accuracy_score_test(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "e5P0BwZFUNDG",
    "outputId": "753837ce-0c68-4d8d-e1e9-a197f761aca3"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=MAX_SEQUENCE_LENGTH, \n",
    "                           trainable=False))\n",
    "\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(units=36, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "SfDTS2zA1AyW",
    "outputId": "b0be4551-28dd-41ce-f1e9-db1c393ba1a3"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DNN_Try.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
